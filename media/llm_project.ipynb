{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\pyplot.py:66\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _docstring\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     65\u001b[0m     FigureCanvasBase, FigureManagerBase, MouseButton)\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfigure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Figure, FigureBase, figaspect\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgridspec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSpec, SubplotSpec\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rcsetup, rcParamsDefault, rcParamsOrig\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\figure.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _blocking_input, backend_bases, _docstring, projections\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     Artist, allow_rasterization, _finalize_rasterization)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     44\u001b[0m     DrawEvent, FigureCanvasBase, NonGuiException, MouseButton, _get_renderer)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\projections\\__init__.py:55\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mNon-separable transforms that map from data space to screen space.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m`matplotlib.projections.polar` may also be of interest.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m axes, _docstring\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AitoffAxes, HammerAxes, LambertAxes, MollweideAxes\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolarAxes\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\axes\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _base\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_axes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axes  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Backcompat.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\axes\\_base.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook, _docstring, offsetbox\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martist\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmartist\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxis\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmaxis\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _OrderedSet, _check_1d, index_of\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmcoll\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\matplotlib\\axis.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmticker\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmtransforms\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munits\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmunits\u001b[39;00m\n\u001b[0;32m     24\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     26\u001b[0m GRIDLINE_INTERPOLATION_STEPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m180\u001b[39m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1138\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1078\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1507\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1479\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1615\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openai\n",
    "import json\n",
    "\n",
    "def query_llm(prompt):\n",
    "    \"\"\"Query the LLM (GPT-4o-Mini) using the AI Proxy token.\"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a data analyst. Provide insights based on the provided dataset summary.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying the LLM: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "def create_visualizations(df):\n",
    "    \"\"\"Generate visualizations from the dataset and save as PNG.\"\"\"\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.savefig(\"correlation_heatmap.png\")\n",
    "    plt.close()\n",
    "\n",
    "    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        plt.figure()\n",
    "        sns.histplot(df[col], kde=True)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.savefig(f\"{col}_distribution.png\")\n",
    "        plt.close()\n",
    "\n",
    "    missing_values = df.isnull().sum()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_values.plot(kind='bar')\n",
    "    plt.title(\"Missing Values Per Column\")\n",
    "    plt.savefig(\"missing_values.png\")\n",
    "    plt.close()\n",
    "\n",
    "def generate_readme(summary_stats, missing_values, llm_response, df):\n",
    "    \"\"\"Generate a README.md file with the analysis results.\"\"\"\n",
    "    with open(\"README.md\", \"w\") as f:\n",
    "        f.write(\"# Automated Data Analysis Report\\n\\n\")\n",
    "        f.write(\"## Dataset Summary\\n\")\n",
    "        f.write(summary_stats.to_markdown() + \"\\n\\n\")\n",
    "        f.write(\"## Missing Values\\n\")\n",
    "        f.write(missing_values.to_markdown() + \"\\n\\n\")\n",
    "        f.write(\"## Insights\\n\")\n",
    "        f.write(llm_response + \"\\n\\n\")\n",
    "        f.write(\"## Visualizations\\n\")\n",
    "        f.write(\"![Correlation Heatmap](correlation_heatmap.png)\\n\")\n",
    "        f.write(\"![Missing Values](missing_values.png)\\n\")\n",
    "        for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "            f.write(f\"![Distribution of {col}]({col}_distribution.png)\\n\")\n",
    "\n",
    "def main():\n",
    "    # Define the dataset file directly here\n",
    "    filename = r\"C:\\Users\\Abhishek R K\\Downloads\\goodreads.csv\"\n",
    "    if not os.path.isfile(filename):\n",
    "        print(f\"Error: File {filename} not found.\")\n",
    "        exit(1) \n",
    "\n",
    "    # Set the API token directly in the script\n",
    "    api_token = \"eyJhbGciOiJIUzI1NiJ9.eyJlbWFpbCI6IjIxZjMwMDIwOTBAZHMuc3R1ZHkuaWl0bS5hYy5pbiJ9.L6vVLu2KA5m0RglcGDmTNyj_0k1PEeTRoBcQynJykCc\"\n",
    "    openai.api_key = api_token\n",
    "\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filename}: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Perform generic analysis\n",
    "    summary_stats = df.describe(include='all')\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # Query LLM for insights\n",
    "    column_info = {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this dataset with the following column information:\n",
    "    {json.dumps(column_info, indent=2)}\n",
    "\n",
    "    Summary statistics:\n",
    "    {summary_stats.to_string()}\n",
    "\n",
    "    Missing values:\n",
    "    {missing_values.to_string()}\n",
    "    \"\"\"\n",
    "    llm_response = query_llm(prompt)\n",
    "\n",
    "    # Create visualizations and generate README\n",
    "    create_visualizations(df)\n",
    "    generate_readme(summary_stats, missing_values, llm_response, df)\n",
    "    print(\"Analysis complete. Results saved in README.md and visualization PNG files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error querying the proxy: 404 Client Error: Not Found for url: https://aiproxy.sanand.workers.dev/v1/chat/completions\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "api_url = \"https://aiproxy.sanand.workers.dev/v1/chat/completions\"\n",
    "api_token = \"eyJhbGciOiJIUzI1NiJ9.eyJlbWFpbCI6IjIxZjMwMDIwOTBAZHMuc3R1ZHkuaWl0bS5hYy5pbiJ9.L6vVLu2KA5m0RglcGDmTNyj_0k1PEeTRoBcQynJykCc\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a data analyst.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Analyze this dataset.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "    print(response.json())\n",
    "except Exception as e:\n",
    "    print(f\"Error querying the proxy: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved in README.md and visualization PNG files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# AI Proxy details\n",
    "API_URL = \"https://aiproxy.sanand.workers.dev/openai/v1/chat/completions\"\n",
    "API_TOKEN = \"eyJhbGciOiJIUzI1NiJ9.eyJlbWFpbCI6IjIxZjMwMDIwOTBAZHMuc3R1ZHkuaWl0bS5hYy5pbiJ9.L6vVLu2KA5m0RglcGDmTNyj_0k1PEeTRoBcQynJykCc\"\n",
    "\n",
    "def query_llm(prompt):\n",
    "    \"\"\"Query the LLM using AI Proxy.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a data analyst. Provide insights based on the provided dataset summary.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()  # Raise an error for HTTP issues\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error querying the LLM: {e}\")\n",
    "        return \"Unable to retrieve insights from the LLM due to an error.\"\n",
    "\n",
    "def create_visualizations(df):\n",
    "    \"\"\"Generate visualizations from the dataset and save as PNG.\"\"\"\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.savefig(\"correlation_heatmap.png\")\n",
    "    plt.close()\n",
    "\n",
    "    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        plt.figure()\n",
    "        sns.histplot(df[col], kde=True)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.savefig(f\"{col}_distribution.png\")\n",
    "        plt.close()\n",
    "\n",
    "    missing_values = df.isnull().sum()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_values.plot(kind='bar')\n",
    "    plt.title(\"Missing Values Per Column\")\n",
    "    plt.savefig(\"missing_values.png\")\n",
    "    plt.close()\n",
    "\n",
    "def generate_readme(summary_stats, missing_values, llm_response, df):\n",
    "    \"\"\"Generate a README.md file with the analysis results.\"\"\"\n",
    "    llm_response = llm_response or \"No insights could be retrieved from the LLM due to an error.\"\n",
    "\n",
    "    with open(\"README.md\", \"w\") as f:\n",
    "        f.write(\"# Automated Data Analysis Report\\n\\n\")\n",
    "        f.write(\"## Dataset Summary\\n\")\n",
    "        f.write(summary_stats.to_markdown() + \"\\n\\n\")\n",
    "        f.write(\"## Missing Values\\n\")\n",
    "        f.write(missing_values.to_markdown() + \"\\n\\n\")\n",
    "        f.write(\"## Insights\\n\")\n",
    "        f.write(llm_response + \"\\n\\n\")\n",
    "        f.write(\"## Visualizations\\n\")\n",
    "        f.write(\"![Correlation Heatmap](correlation_heatmap.png)\\n\")\n",
    "        f.write(\"![Missing Values](missing_values.png)\\n\")\n",
    "        for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "            f.write(f\"![Distribution of {col}]({col}_distribution.png)\\n\")\n",
    "\n",
    "def main():\n",
    "    # Define the dataset file directly here\n",
    "    filename = r\"C:\\Users\\Abhishek R K\\Downloads\\goodreads.csv\"\n",
    "    if not os.path.isfile(filename):\n",
    "        print(f\"Error: File {filename} not found.\")\n",
    "        exit(1)\n",
    "\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filename}: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Perform generic analysis\n",
    "    summary_stats = df.describe(include='all')\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # Query LLM for insights\n",
    "    column_info = {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this dataset with the following column information:\n",
    "    {json.dumps(column_info, indent=2)}\n",
    "\n",
    "    Summary statistics:\n",
    "    {summary_stats.to_string()}\n",
    "\n",
    "    Missing values:\n",
    "    {missing_values.to_string()}\n",
    "    \"\"\"\n",
    "    llm_response = query_llm(prompt)\n",
    "\n",
    "    # Create visualizations and generate README\n",
    "    create_visualizations(df)\n",
    "    generate_readme(summary_stats, missing_values, llm_response, df)\n",
    "    print(\"Analysis complete. Results saved in README.md and visualization PNG files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MERA LUND SABSE BADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved in README.md and visualization PNG files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# AI Proxy details\n",
    "API_URL = \"https://aiproxy.sanand.workers.dev/openai/v1/chat/completions\"\n",
    "API_TOKEN = \"eyJhbGciOiJIUzI1NiJ9.eyJlbWFpbCI6IjIxZjMwMDIwOTBAZHMuc3R1ZHkuaWl0bS5hYy5pbiJ9.L6vVLu2KA5m0RglcGDmTNyj_0k1PEeTRoBcQynJykCc\"\n",
    "\n",
    "def query_llm(prompt):\n",
    "    \"\"\"Query the LLM using AI Proxy.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a data analyst. Provide insights based on the provided dataset summary.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()  # Raise an error for HTTP issues\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error querying the LLM: {e}\")\n",
    "        return \"Unable to retrieve insights from the LLM due to an error.\"\n",
    "\n",
    "def detect_outliers(df):\n",
    "    \"\"\"Detect outliers using z-scores.\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    outliers = {}\n",
    "    for col in numeric_cols:\n",
    "        z_scores = zscore(df[col].dropna())\n",
    "        z_scores = pd.Series(z_scores, index=df[col].dropna().index)\n",
    "        outliers[col] = df.loc[z_scores[np.abs(z_scores) > 3].index]\n",
    "    return outliers\n",
    "\n",
    "def perform_clustering(df):\n",
    "    \"\"\"Perform k-means clustering on numerical data.\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64'])\n",
    "    if len(numeric_cols.columns) < 2:\n",
    "        return None  # Clustering requires at least 2 features\n",
    "\n",
    "    numeric_data = numeric_cols.dropna()\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    clusters = kmeans.fit_predict(numeric_data)\n",
    "\n",
    "    # Create a cluster column with NaN for rows with missing values\n",
    "    df['Cluster'] = np.nan\n",
    "    df.loc[numeric_data.index, 'Cluster'] = clusters\n",
    "\n",
    "    return df, kmeans.cluster_centers_\n",
    "\n",
    "def feature_importance(df, target_col):\n",
    "    \"\"\"Identify feature importance using a random forest regressor.\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).drop(columns=[target_col], errors='ignore')\n",
    "    target = df[target_col]\n",
    "    if numeric_cols.empty or target.isnull().all():\n",
    "        return None\n",
    "\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    model.fit(numeric_cols.fillna(0), target.fillna(0))\n",
    "    importance = pd.Series(model.feature_importances_, index=numeric_cols.columns)\n",
    "    return importance.sort_values(ascending=False)\n",
    "\n",
    "def create_visualizations(df):\n",
    "    \"\"\"Generate visualizations from the dataset and save as PNG.\"\"\"\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.savefig(\"correlation_heatmap.png\")\n",
    "    plt.close()\n",
    "\n",
    "    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        plt.figure()\n",
    "        sns.histplot(df[col], kde=True)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.savefig(f\"{col}_distribution.png\")\n",
    "        plt.close()\n",
    "\n",
    "    missing_values = df.isnull().sum()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_values.plot(kind='bar')\n",
    "    plt.title(\"Missing Values Per Column\")\n",
    "    plt.savefig(\"missing_values.png\")\n",
    "    plt.close()\n",
    "\n",
    "def generate_readme(summary_stats, missing_values, llm_response, df):\n",
    "    \"\"\"Generate a README.md file with the analysis results.\"\"\"\n",
    "    llm_response = llm_response or \"No insights could be retrieved from the LLM due to an error.\"\n",
    "\n",
    "    with open(\"README.md\", \"w\") as f:\n",
    "        f.write(\"# Automated Data Analysis Report\\n\\n\")\n",
    "        f.write(\"## Dataset Summary\\n\")\n",
    "        f.write(summary_stats.to_markdown() + \"\\n\\n\")\n",
    "        f.write(\"## Missing Values\\n\")\n",
    "        f.write(missing_values.to_markdown() + \"\\n\\n\")\n",
    "        f.write(\"## Insights\\n\")\n",
    "        f.write(llm_response + \"\\n\\n\")\n",
    "        f.write(\"## Visualizations\\n\")\n",
    "        f.write(\"![Correlation Heatmap](correlation_heatmap.png)\\n\")\n",
    "        f.write(\"![Missing Values](missing_values.png)\\n\")\n",
    "        for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "            f.write(f\"![Distribution of {col}]({col}_distribution.png)\\n\")\n",
    "\n",
    "def main():\n",
    "    # Define the dataset file directly here\n",
    "    filename = r\"C:\\Users\\Abhishek R K\\Downloads\\happiness.csv\"\n",
    "    if not os.path.isfile(filename):\n",
    "        print(f\"Error: File {filename} not found.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(filename, encoding=\"ISO-8859-1\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filename}: {e}\")\n",
    "        return\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        print(\"The dataset is empty or could not be loaded.\")\n",
    "        return\n",
    "\n",
    "    # Perform generic analysis\n",
    "    summary_stats = df.describe(include='all')\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # Detect outliers\n",
    "    outliers = detect_outliers(df)\n",
    "\n",
    "    # Perform clustering\n",
    "    clustering_result = perform_clustering(df)\n",
    "    if clustering_result:\n",
    "        clustered_df, cluster_centers = clustering_result\n",
    "    else:\n",
    "        print(\"Clustering could not be performed due to insufficient data.\")\n",
    "\n",
    "    # Query LLM for insights\n",
    "    column_info = {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this dataset with the following column information:\n",
    "    {json.dumps(column_info, indent=2)}\n",
    "\n",
    "    Summary statistics:\n",
    "    {summary_stats.to_string()}\n",
    "\n",
    "    Missing values:\n",
    "    {missing_values.to_string()}\n",
    "\n",
    "    Outliers detected:\n",
    "    {outliers}\n",
    "    \"\"\"\n",
    "    llm_response = query_llm(prompt)\n",
    "\n",
    "    # Create visualizations and generate README\n",
    "    create_visualizations(df)\n",
    "    generate_readme(summary_stats, missing_values, llm_response, df)\n",
    "    print(\"Analysis complete. Results saved in README.md and visualization PNG files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mine's clearly bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved in README.md and visualization PNG files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# AI Proxy details\n",
    "API_URL = \"https://aiproxy.sanand.workers.dev/openai/v1/chat/completions\"\n",
    "API_TOKEN = \"eyJhbGciOiJIUzI1NiJ9.eyJlbWFpbCI6IjIxZjMwMDIwOTBAZHMuc3R1ZHkuaWl0bS5hYy5pbiJ9.L6vVLu2KA5m0RglcGDmTNyj_0k1PEeTRoBcQynJykCc\"\n",
    "\n",
    "\n",
    "def query_llm(prompt):\n",
    "    \"\"\"Query the LLM using AI Proxy.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a data analyst. Provide insights based on the provided dataset summary.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()  # Raise an error for HTTP issues\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error querying the LLM: {e}\")\n",
    "        return \"Unable to retrieve insights from the LLM due to an error.\"\n",
    "\n",
    "def detect_outliers(df):\n",
    "    \"\"\"Detect outliers using z-scores.\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    outliers = {}\n",
    "    for col in numeric_cols:\n",
    "        z_scores = zscore(df[col].dropna())\n",
    "        z_scores = pd.Series(z_scores, index=df[col].dropna().index)\n",
    "        outliers[col] = df.loc[z_scores[np.abs(z_scores) > 3].index]\n",
    "    return outliers\n",
    "\n",
    "def perform_clustering(df):\n",
    "    \"\"\"Perform k-means clustering on numerical data.\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64'])\n",
    "    if len(numeric_cols.columns) < 2:\n",
    "        return None  # Clustering requires at least 2 features\n",
    "\n",
    "    numeric_data = numeric_cols.dropna()\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    clusters = kmeans.fit_predict(numeric_data)\n",
    "\n",
    "    # Create a cluster column with NaN for rows with missing values\n",
    "    df['Cluster'] = np.nan\n",
    "    df.loc[numeric_data.index, 'Cluster'] = clusters\n",
    "\n",
    "    return df, kmeans.cluster_centers_\n",
    "\n",
    "def feature_importance(df, target_col):\n",
    "    \"\"\"Identify feature importance using a random forest regressor.\"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).drop(columns=[target_col], errors='ignore')\n",
    "    target = df[target_col]\n",
    "    if numeric_cols.empty or target.isnull().all():\n",
    "        return None\n",
    "\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    model.fit(numeric_cols.fillna(0), target.fillna(0))\n",
    "    importance = pd.Series(model.feature_importances_, index=numeric_cols.columns)\n",
    "    return importance.sort_values(ascending=False)\n",
    "\n",
    "def create_visualizations(df):\n",
    "    \"\"\"Generate visualizations from the dataset and save as PNG.\"\"\"\n",
    "    correlation_matrix = df.corr(numeric_only=True)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.savefig(\"correlation_heatmap.png\")\n",
    "    plt.close()\n",
    "\n",
    "    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        plt.figure()\n",
    "        sns.histplot(df[col], kde=True)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.savefig(f\"{col}_distribution.png\")\n",
    "        plt.close()\n",
    "\n",
    "    missing_values = df.isnull().sum()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_values.plot(kind='bar')\n",
    "    plt.title(\"Missing Values Per Column\")\n",
    "    plt.savefig(\"missing_values.png\")\n",
    "    plt.close()\n",
    "\n",
    "def generate_readme(summary_stats, missing_values, llm_response, df):\n",
    "    \"\"\"Generate a README.md file with the analysis results.\"\"\"\n",
    "    llm_response = llm_response or \"No insights could be retrieved from the LLM due to an error.\"\n",
    "\n",
    "    with open(\"README.md\", \"w\") as f:\n",
    "        f.write(\"# Automated Data Analysis Report\\n\\n\")\n",
    "        f.write(\"## Dataset Summary\\n\")\n",
    "        f.write(summary_stats.to_markdown() + \"\\n\\n\")\n",
    "        f.write(\"## Missing Values\\n\")\n",
    "        f.write(missing_values.to_markdown() + \"\\n\\n\")\n",
    "        f.write(\"## Insights\\n\")\n",
    "        f.write(llm_response + \"\\n\\n\")\n",
    "        f.write(\"## Visualizations\\n\")\n",
    "        f.write(\"![Correlation Heatmap](correlation_heatmap.png)\\n\")\n",
    "        f.write(\"![Missing Values](missing_values.png)\\n\")\n",
    "        for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "            f.write(f\"![Distribution of {col}]({col}_distribution.png)\\n\")\n",
    "\n",
    "def main():\n",
    "    # Define the dataset file directly here\n",
    "    filename = r\"C:\\Users\\Abhishek R K\\Downloads\\media.csv\"\n",
    "    if not os.path.isfile(filename):\n",
    "        print(f\"Error: File {filename} not found.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv(filename, encoding=\"ISO-8859-1\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filename}: {e}\")\n",
    "        return\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        print(\"The dataset is empty or could not be loaded.\")\n",
    "        return\n",
    "\n",
    "    # Perform generic analysis\n",
    "    summary_stats = df.describe(include='all')\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # Detect outliers\n",
    "    outliers = detect_outliers(df)\n",
    "\n",
    "    # Perform clustering\n",
    "    clustering_result = perform_clustering(df)\n",
    "    if clustering_result:\n",
    "        clustered_df, cluster_centers = clustering_result\n",
    "    else:\n",
    "        print(\"Clustering could not be performed due to insufficient data.\")\n",
    "\n",
    "    # Query LLM for insights\n",
    "    column_info = {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this dataset with the following column information:\n",
    "    {json.dumps(column_info, indent=2)}\n",
    "\n",
    "    Summary statistics:\n",
    "    {summary_stats.to_string()}\n",
    "\n",
    "    Missing values:\n",
    "    {missing_values.to_string()}\n",
    "\n",
    "    Outliers detected:\n",
    "    {outliers}\n",
    "    \"\"\"\n",
    "    llm_response = query_llm(prompt)\n",
    "\n",
    "    # Create visualizations and generate README\n",
    "    create_visualizations(df)\n",
    "    generate_readme(summary_stats, missing_values, llm_response, df)\n",
    "    print(\"Analysis complete. Results saved in README.md and visualization PNG files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
